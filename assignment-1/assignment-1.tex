\def\Module{Principles of Computer System Design}
\def\Uebung{Assignment 1}
\def\Studentenname{Marcus Voss (qcz284), R. Schmidtke (rxt809), Marco Eilers (dbk726)}
\def\Sub_date{04.12.2012}

\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fullpage} 
\headsep1cm
\parindent0cm
\usepackage{amssymb, amstext, amsmath}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}

\lhead{\textbf{\Module}}
\rhead{\Uebung~(Submission: \Sub_date)}

\cfoot{}
\lfoot{\Studentenname}
\rfoot{\thepage\ of \pageref{LastPage}}
\pagestyle{fancy}
\renewcommand{\footrulewidth}{0.4pt}

\newcommand{\code}[1]{{\fontfamily{fvm}\small \selectfont #1}}

%Line spacing between paragraphs
\setlength{\parskip}{6pt}

\begin{document}

\section*{Exercises} 
\label{sec:exercises}

\subsection*{Question 1}
\label{sec:eq1}

\subsection*{Question 2}
\label{sec:eq2}
Le Voss!

\subsection*{Question 3}
\label{sec:eq3}
(a) How does concurrency influence latency in a computer system? Is its influence always positive,
always negative, or is there a trade-off? Explain. (2 paragraphs)
(b) Explain the difference between dallying and batching. Provide one example of each. (2
paragraphs)
(c) Is caching an example of a fast path optimization? Explain why or why not. (1 paragraph)

\subsubsection*{(a)}
While concurrency usually has a positive effect on a system's latency, there may also be cases where this effect is only marginal or even negative. In general, the ability to process several requests at once means that some requests can be worked on right away instead of having to wait for the completion of another request, or at least that qaiting time is reduced. However, this is only true if the system's utilizaztion is high enough. If, on the other hand, the system's utilization is low and it only ever has one request at a time do work with, concurrency will not have a positive effect at all. In this case, it might even have a slightly negative effect, since the added overhead for work distribution and scheduling will add to the latency of every single request.

In the above paragraph, we assumed that the system's hardware natively supports concurrency, which is usually the case nowadays, so that several units of work can actually be executed at the same time. If, however, the system only simulates concurrent execution, the impact on the latency will quite probably always be negative. In this case the system would start working on several tasks at once and distribute the processor's computing time between them. While fewer (or no) tasks would have to wait before they are started, the work on every task will take considerably longer. Combined with the overhead from scheduling etc., the overall latency would be at least as high as with serial execution.

\subsubsection*{(b)}
Batching means performing a number of related tasks at the same time instead of doing each one on its own, usually in order to reduce the overall overhead: Often the overhead for specific tasks stays the same, independent of the size of the task. In this case it can be advantageous to combine several tasks into one, so that the overhead occurs only once and not once for every task. There may be additional performance benefits that arise from similarities or a certain order of the tasks. 

Dallying, on the other hand, means that one delays the execution of a task to get some kind of performance benefit. One reason to do this is because it might not actually be necessary to execute the task after all (but this is not yet clear). The other reason is related to batching: One delays the execution of several small tasks and waits until a certain number of such tasks has been collected. This collection of tasks is then executed at once (using batch execution) in order to profit from the reduced overhead, as described above. Batching may therefore be a part of (or a motivation for) dallying.

Batching is quite common in computing as well as in the real world. TODO

\subsubsection*{(c)}
Caching is a perfect example for fast path optimization. It means that, in addition to the normal and relatively slow way of accessing data, which usually means accessing RAM or a disk or network resource, there is another, much faster way for common requests. This faster way does not work for all kinds of requests (otherwise one would not use the slower one at all), in our case because a cache is typically at least an order of magnitude smaller than the actual memory. A good cache therefore contains those parts of the memory that are accessed most frequently, and getting data from the cache is much faster than getting them from the actual memory. 

\subsection*{Question 4}
\label{sec:eq4}
Marco

\section*{Programming Task}
\label{sec:programming}

\subsection*{Question 1}
\label{sec:pq1}
There are basically three types of semantics: 'Exactly once', 'at most once' and 'at least once'. Since all of these three types rely on the interaction of client and server, it depends mostly on the client what type of semantics is employed (as it may choose to try again and again). The service and client implementations we provide are of the type 'at most once'. If the service is unavailable at the time of the request, the request is not repeated, but an exception is raised. Similarly, exceptions are raised on other errors during execution on the server (as per the \code{KeyValueBase} interface). The clients we ship respect that and do not try again, so the overall semantics used are of type 'at most once'. Other client implementations may prefer to retry the operation on failure until it has succeeded (at least) once which can be implemented just fine by keeping track of errors from the service.

\subsection*{Question 2}
\label{sec:pq2}

Marco

\subsection*{Question 3}
\label{sec:pq3}

Vossi / Marco

\subsection*{Question 4}
\label{sec:pq4}

\subsection*{Question 5}
\label{sec:pq5}

data set used is\footnote{Downloaded from: \url{http://an.kaist.ac.kr/~haewoon/release/twitter_social_graph/} [last accessed on: November 30, 2012]}

\subsection*{Question 6}
\label{sec:pq6}
Vossi

\subsection*{Question 7}
\label{sec:pq7} 
Vossi

\subsection*{Question 8}
\label{sec:pq8}
Marco

\subsection*{Question 9}
\label{sec:pq9}
Kaniner!

\end{document}
