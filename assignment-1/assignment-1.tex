\def\Module{Principles of Computer System Design}
\def\Uebung{Assignment 1}
\def\Studentenname{Marcus Voss (qcz284), R. Schmidtke (rxt809), Marco Eilers (dbk726)}
\def\Sub_date{04.12.2012}

\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fullpage} 
\headsep1cm
\parindent0cm
\usepackage{amssymb, amstext, amsmath}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{threeparttable}
\usepackage{footnote}
\makesavenoteenv{tabular}

\lhead{\textbf{\Module}}
\rhead{\Uebung~(Submission: \Sub_date)}

\cfoot{}
\lfoot{\Studentenname}
\rfoot{\thepage\ of \pageref{LastPage}}
\pagestyle{fancy}
\renewcommand{\footrulewidth}{0.4pt}

\newcommand{\code}[1]{{\fontfamily{fvm}\small \selectfont #1}}

%Line spacing between paragraphs
\setlength{\parskip}{6pt}

\begin{document}

\section*{Exercises} 
\label{sec:exercises}

\subsection*{Question 1}
\label{sec:eq1}
\subsubsection*{(a)}
Our abstraction would have one central component which receives all read/write requests from the clients and translates them into one or several requests for the individual machines, waits for the result, combines the results if necessary and sends it back to the clients. All information concerning the abstraction would be kept in this component; neither the clients nor the individual machines have to know that there is any abstraction at all. We could translate addresses by assigning an offset to each of the storage machines: the first machine would have the offset zero, and the $n$th machine would have the offset $offset(n-1) + storageSize(n-1)$. Computing the ID of the machine and the physical memory address from an address in the global address space would then mean finding the two offsets between which the address lies and subtracting the lower offset from it.
This can be simplified if all storage machines provide the same amount of storage and the size of this storage is a power of two; in this case the first few bits of the global address will be the ID of the machine, and the following bits denote the physical address, so that only two simple bitwise operations are necessary.

The central component would also have to regularly check if all machines are still online, and inform clients when their requests cannot be fulfilled because a machine needed for their completion cannot be reached. 

In this scenario, all traffic has to pass the central component for every transaction, so this component is an obvious bottleneck. It would therefore be a good idea to have several instances of this central component. Each could work on its own, so there are no synchronization issues. A load balancer could be used to make sure that all such components are used equally. In this case, the storage should scale very well since more manager components can be added at will. With only one such component, however, the whole storage's performance is would be limited by the central component and would not scale at all after this point.
\subsubsection*{(b)}
TODO
(b) Show the pseudocode and API of the READ and WRITE functions of your memory abstraction.
Note that your pseudocode should show both translation of addresses (naming) as well as
communicate with individual machines to obtain the actual data. (for each function: API + 1-2
sentence description of API + pseudocode)
\begin{itemize}
  \item \texttt{getStorageAttributes()}
  \item \texttt{getOffsets()}
  \item \texttt{read(long offset, int length)}
  \item \texttt{write(long offset, byte[] data)}
\end{itemize}


\subsubsection*{(c)}
Simple operations against main memory are TODO atomic. For our memory abstraction, we would definitely make basic operations non-atomic (unless we know for certain that it will only ever be used for purposes that require atomicity). Guaranteeing atomicity always comes with a performance cost, and a basic component like memory should offer the possibility to read and write as fast as possible for cases where atomicity is not needed. It is always possible to make wrappers for existing functions that do guarantee atomicity (simply by implementing read and write locks for parts of the memory, e.g. a lock for every page), whereas it is not possible to get rid of the overhead if atomicity is implemented in the memory (abstraction) itself.

\subsection*{Question 2}
\label{sec:eq2}

\subsubsection*{(a)}
\begin{table}[htbp]
\caption{Comparison of Storage}
\begin{center}
\begin{tabular}{|p{0.35\textwidth}|p{0.18\textwidth}|p{0.18\textwidth}|p{0.18\textwidth}|}
\hline
 & Hard disk & SSD & Main Memory \\ \hline
1 Access time read/write & 6.78 ms/8.83 ms & 0.04 ms/0.05 ms & 100 ns/100 ns \\ \hline
2 Average Capacity & 1 TB & 128GB & 16GB \\ \hline
3 Cost per unit & 267.59\$ ($\frac{0.26\$}{GB}$) & 109.99\$ ($\frac{0.86\$}{GB}$) & 390.00\$ ($\frac{24.37\$}{GB}$) \\ \hline
4 Reliability (MTBF, volatility) & 1-1.5 mio. hours, non-volatile & 1-2 mio. hours, non-volatile & 6-11 mio. hours, versatile \\ \hline
5 Power Consumption idle/typical/max write (per h) & 4/6.5/5.5 watt& 0.3/1.3/2.5 watt & about 4 watt \\ \hline
\end{tabular}
\end{center}
\label{tab:storage}
\end{table}

Table \ref{tab:storage} shows evaluation numbers obtained from a web search. For hard disk (HDD) and solid state disk (SSD) the numbers for dimensions 1-3 and 5 are well comparable as they are from the same source. Numbers where taken from the fastest (access reading time) hard disk\footnote{\url{http://www.tomshardware.com/charts/hdd-charts-2012/compare,2898.html?prod\%5B5531\%5D=on} [last accessed: 03.12.2012]} and SSD\footnote{\url{http://www.tomshardware.com/charts/ssd-charts-2012/compare,2788.html?prod\%5B5837\%5D=on} [last accessed: 03.12.2012]} in the respective benchmark test for each of the categories\footnote{\emph{HDD Charts 2012} and \emph{SSD Charts 2012}}. These numbers are however for desktop systems, but we can assume that it is in the same range for server systems. To assess reliability in terms of media failure, a common measure is Mean Time Before Failure (MTBF, but also found as Mean Time To Failure). The figures for the HDD is from \cite{Schroeder2007} and gives a range that is typically stated to be 1-1.5 mio. hours\footnote{Although the paper analyses these claims and finds that these numbers are mostly exaggerated}. For SSD we could find a similar range of about 1-2 mio. hours\footnote{\url{http://solid-state-drive-review.toptenreviews.com/} [last accessed: 03.12.2012]}. For reliability we want to further distinguish between volatility and non-volatility, as in the case of a crash the data in RAM is generally lost, while it is still there in HDD and SSD. Finding specific and reliable numbers for main memory (RAM) was not as easy as for the storage disks, as they are benchmarked in different ways and with different measures. Access times are not found that specifically. However, they are in a much smaller range. While disk access times (SSD and HDD) are reported in milliseconds ($10^{-3}$), they are in the range of nanoseconds for RAM ($10^{-9}$), as found in \cite{Dean}. Figures for reliability and power consumption where not found from reliable sources, and are only estimates based on posts in forums\footnote{MTBF: \url{http://www.theriac.org/forum/showthread.php?t=12954} and power consumption: \url{http://answers.yahoo.com/question/index?qid=20100711051719AAVSIhe},
\url{http://www.tomshardware.com/reviews/lovo-ddr3-power,2650.html} [all last accessed: 03.12.2012]}. Also the size and price ranges depend a lot on the type of system. We chose rather randomly a representative example\footnote{\url{http://h30094.www3.hp.com/product.aspx?sku=10239599} [last accessed: 03.12.2012]}.

\subsubsection*{(b)}

As shown in \emph{a)} the speed of RAM still a lot faster than the disk based memories - even when compared to the already fast SSD (e.g. about 400 times faster for our numbers). However, the cost per GB is much higher for RAM when compared to the disk based memories (with our numbers almost 100 times more expensive than HDD, and almost 30 times more than SSD). So when designing a distributed memory abstraction such as above, a first trade off is between price of storage and speed. The HDD is currently the cheapest memory, but very slow, when compared to SDD, and especially RAM. But the SSD could certainly become an alternative to consider, especially as its costs are currently decreasing, and the common disk sizes are increasing, so that the gap to HDD in regard to these dimension becomes smaller and smaller - with a vast improvement in the speed dimension. In terms of reliability there were no significant differences in terms of failures, but the disk based memories are non-volatile! So deploying e.g. SSDs as part of the above system could increase overall reliability of the system if crucial parts, such as logs for recovery, are stored on SSD. In the dimension of power consumption we could also find no significant differences between these technologies. Here other parts of the system such as the CPU are rather worth investing in to lower power consumption.

\subsection*{Question 3}
\label{sec:eq3}

\subsubsection*{(a)}
While concurrency usually has a positive effect on a system's latency, there may also be cases where this effect is only marginal or even negative. In general, the ability to process several requests at once means that some requests can be worked on right away instead of having to wait for the completion of another request, or at least that waiting time is reduced. However, this is only true if the system's utilization is high enough. If, on the other hand, the system's utilization is low and it only ever has one request at a time do work with, concurrency will not have a positive effect at all. In this case, it might even have a slightly negative effect, since the added overhead for work distribution and scheduling will add to the latency of every single request.

In the above paragraph, we assumed that the system's hardware natively supports concurrency, which is usually the case nowadays, so that several units of work can actually be executed at the same time. If, however, the system only simulates concurrent execution, the impact on the latency will quite probably always be negative. In this case the system would start working on several tasks at once and distribute the processor's computing time between them. While fewer (or no) tasks would have to wait before they are started, the work on every task will take considerably longer. Combined with the overhead from scheduling etc., the overall latency would be at least as high as with serial execution.

\subsubsection*{(b)}
Batching means performing a number of related tasks at the same time instead of doing each one on its own, usually in order to reduce the overall overhead: Often the overhead for specific tasks stays the same, independent of the size of the task. In this case it can be advantageous to combine several tasks into one, so that the overhead occurs only once and not once for every task. There may be additional performance benefits that arise from similarities or a certain order of the tasks. 

Dallying, on the other hand, means that one delays the execution of a task to get some kind of performance benefit. One reason to do this is because it might not actually be necessary to execute the task after all (but this is not yet clear). The other reason is related to batching: One delays the execution of several small tasks and waits until a certain number of such tasks has been collected. This collection of tasks is then executed at once (using batch execution) in order to profit from the reduced overhead, as described above. Batching may therefore be a part of (or a motivation for) dallying.

Both batching and dallying are quite common in computing as well as in the real world. Message passing is an example where batching can reduce the overall overhead considerably, since the static overhead for sending one message is usually much lower than for sending several smaller ones. An example where dallying could be used is our key-value-store: if there are several operations on a single key, one of which negates the effects of a previous one (e.g. overwrote or deletes the value), then the operation(s) before the last operation can be left our. A real world example where dallying is used to use batching is washing dishes: If one waits for a few days until washing dishes again, one can wash more plates at once, and therefore the overhead of letting water into the sink etc. occurs only once instead of several times.

\subsubsection*{(c)}
Caching is a perfect example for fast path optimization. It means that, in addition to the normal and relatively slow way of accessing data, which usually means accessing RAM or a disk or network resource, there is another, much faster way for common requests. This faster way does not work for all kinds of requests (otherwise one would not use the slower one at all), in our case because a cache is typically at least an order of magnitude smaller than the actual memory. A good cache therefore contains those parts of the memory that are accessed most frequently, and getting data from the cache is much faster than getting them from the actual memory. 

\subsection*{Question 4}
\label{sec:eq4}

\subsubsection*{(a)}
Typical caches use two properties to decide which parts of the memory to keep in the cache: temporal locality and spatial locality. This means that data is cached if a) it has been requested in the (recent) past or b) it is physically close to data that has already been requested. Therefore we would need to write a test program that selects an area in memory that fits into the cache and repeatedly accesses parts of this area for some time before starting the measurement. We expect that this area would then be cached, and that all subsequent operations in this area would be cache hits.

Then there are several things we could to to actually measure the memory bandwidth. We could copy data from one part of the memory to another part. If we want to measure the raw bandwidth, we would typically copy large chunks of memory and measure how long this takes. We could use the same setup to measure if the performance differs between reads and writes, for example by writing zeroes all over the cached memory area, with no reads at all. Another metric we could measure is cache access latency: here we would only move small chunks of data, but many of them, so that the (variable) time needed for actually moving the data becomes less important than the (static) time needed for requesting and responding.

In all cases, we have to make sure that our program is the only one using the machine at the time of the experiment. Since we are basically doing some brainless copying, we also have to make sure that our compiler (or VM) does not optimize any operations away.

\subsubsection*{(b)}
To ensure that all our operations miss the cache, we can do something similar to the procedure described above. Again, we start out by repeatedly reading a certain area in memory, expecting that it will be cached. This time, however, we are doing this to ensure that other areas in the memory are not cached, thus the memory area we access must be big enough to actually fill all or at least most of the cache. In all operations during the actual measurement, we would then use parts of the memory that are \emph{outside} this area. While doing this, the cache will be refilled with the data we access during the measurement, so we also need to make sure that we do not access the same area several times, or even an area right after an area that we accessed before.

Although we are deliberately accessing data that has not been used in some time, we still want to make sure that this data actually resides in main memory and has not been swapped to a disk, since any disk access would mean that we also measure time that has nothing to do with moving data from the memory to the CPU and back. We must therefore make sure that our overall RAM usage does not exceed the amount of RAM that is available on our machine.

\section*{Programming Task}
\label{sec:programming}

\subsection*{Question 1}
\label{sec:pq1}
There are basically three types of semantics: 'Exactly once', 'at most once' and 'at least once'. Since all of these three types rely on the interaction of client and server, it depends mostly on the client what type of semantics is employed (as it may choose to try again and again). The service and client implementations we provide are of the type 'at most once'. If the service is unavailable at the time of the request, the request is not repeated, but an exception is raised. Similarly, exceptions are raised on other errors during execution on the server (as per the \code{KeyValueBase} interface). The clients we ship respect that and do not try again, so the overall semantics used are of type 'at most once'. Other client implementations may prefer to retry the operation on failure until it has succeeded (at least) once which can be implemented just fine by keeping track of errors from the service.

\subsection*{Question 2}
\label{sec:pq2}

We do all memory management and locking and most of the store's logic in IndexImpl, which is a singleton. The current key-value-pairs are managed in a Hashtable, which maps keys to a SpaceIdent object, which in turn contains a value's position and length in the memory mapped file. We also have a list of SpaceIdents which specify all empty areas in the store, i.e. areas that may be overwritten and used by new values. Space is declared to be free when a value is deleted, or when it is updated with a value which does not fit into the currently allocated space. In the latter case, the old space is marked as free and a new free area is found which is large enough to contain the new value. If no such place is found in the emptyList, the new value is spaced at the end of the currently used space. The same happens with inserts. The emptyList is sorted by the SpaceIdent's location. In order to avoid fragmentation of the store, adjacent free areas are joined every time free space is requested (since this requires a traversal of the emptyList anyway). The actual memory mapped file is stored in a subdirectory of the temporary path denoted by the Java environment variable 'java.io.tmpdir', which may be configured in the application server the service is deployed in. The subdirectory is 'dk/diku/pcsd/assignment1/impl/' and the file will be called 'store.mmf'. If it already exists, that file will be used, if it does not exist, a sparse 24GB file will be created (so by supplying a rather small 'store.mmf' file you can decrease the amount of values that can be stored).

We use Java's ReadWriteLock API for locking. The KeyImpl class manages another Hashtable which maps each key to its own ReadWriteLock object, so that each key can be locked on its own and we can distinguish between read and write access. We hope that this granular locking will result in maximal concurrency and therefore maximal performance for our store. When inserting new completely new keys, which do not have a lock yet, we use Java's \texttt{synchronized} on the new key to avoid creating duplicate locks for the same key. In all cases, we acquire all necessary locks at the start of a method and release them again at the end (Conservative Strict 2PL), because we did not want to deal with deadlocks or cascading aborts. For tasks larger than a single key, e.g. atomicScan, we simply acquire the locks for all keys in the order of the keys, while also locking the mapping itself, so that no new keys can be inserted during the operation.

\subsection*{Question 3}
\label{sec:pq3}
To continuously and consistently test our implementation we created a set of unit tests:
\begin{itemize}
\item SimpleReadWriteTest(N): This test simulates only one client that first inserts N values into the key-value store, and does then N sequential random updates and reads. For every read it checks if the values corresponds to a value that is kept locally in a Java Hash Map. The test fails if any read deviates from the expected value.
\item MultiReadTest(N, h, n): This test simulates h clients (threads). First N values are (sequentially) inserted into the key-value store. Then the h clients make n random reads. For every read each thread checks if the values corresponds to a value that is kept in a shared Java Hash Map. The test fails if any read deviates from the expected value.
\item MultiReadWriteTest(N, h, n) This test is similar to MultiReadTest(), but it will also do n random writes and after each write 10 random reads. As above the test fails if any read deviates from the expected value.
\item AtomicUpdateTest(): To test if an update is performed we artificially slowed down the update method in \emph{IndexImpl.java} by inserting a sleep of 10 seconds. Then the test case starts a thread that updates a value in the store, waits a second, and starts a thread that reads that same key. The test succeeds if the read waits for the update to complete (the read value equals the updated value).
\item ScanTest(): This test first inserts a couple thousand keys into the store and then performs scans and atomicScans in a separate thread. Then other threads are spawned that perform update and delete operations in parallel.

First, a regular scan is started and an update thread is started in parallel that modifies values such that they would no longer be recognized by the predicate used in the scan. The expected behaviour is that scan does not return all values that have been inserted initially, but a little less because update operates in parallel. The same behaviour is expected in another test case where instead of parallel update we run parallel delete on the keys that have been previously inserted. As expected, in both cases we get a little less values returned than initially inserted since scan is not atomic.

On the other hand, when running the same scenarios on atomicScan, we expect the exact number of values to be returned that have been initially inserted since no update or delete operations can run in parallel with atomic scan. By means of locking we enforce this behaviour, which is also observed in the test cases.
\end{itemize}


Vossi

\subsection*{Question 4}
\label{sec:pq4}
Robert

\subsection*{Question 5}
\label{sec:pq5}
Robert

\subsection*{Question 6}
\label{sec:pq6}
Vossi

data set used is\footnote{Downloaded from: \url{http://an.kaist.ac.kr/~haewoon/release/twitter_social_graph/} [last accessed on: November 30, 2012]}

\subsection*{Question 7}
\label{sec:pq7} 
Vossi

\subsection*{Question 8}
\label{sec:pq8}
Marco

\subsection*{Question 9}
\label{sec:pq9}
Out implementation of \texttt{atomicScan} differs from the normal \texttt{scan} only in one way. At the start of the function, we first acquire a read lock on the key-to-space-map, then we get read locks for all keys that are currently in the map. We do this in order, from the smallest key to the largest, in order to avoid deadlocks. Once we have all the locks, we execute the scan, i.e. we iterate through all keys, check if they are in the specified range, and return a list of those that are. The evaluation of the predicate is implemented in KeyValueBaseImpl (as opposed to the rest, which is in IndexImpl) and is identical to that of regular scan. Finally, we release the locks on all the keys and subsequently the lock on the map.

For \texttt{bulkPut}, our approach is very similar. Here we get a write lock for the key-to-space-map and subsequently for all keys that are currently in the map. Then we iterate through the supplied data, check if a given key is already present in the store and choose whether to insert or update. At the end, we release all locks on keys and finally the lock on the map. This is not optimal, since we actually only need read locks on the map and the keys during most of the procedure: write locks are needed only for the keys whose values are changed and only when they are changed. The same is true for the mapping. However, Java's ReadWriteLock does not allow the user to upgrade a read lock to a write lock. We would therefore have to release our read locks and request a write lock before every write. This is obviously unacceptable, since it may destroy the atomicity of bulkPut. Therefore we had to use write locks throughout, even though this obviously means that no other operations can be executed during a bulkPut.
 
\begin{thebibliography}{1}

\bibitem[Schroeder2007]{Schroeder2007} Bianca Schroeder and Garth A. Gibson. 2007. Disk failures in the real world: what does an MTTF of 1,000,000 hours mean to you?. In Proceedings of the 5th USENIX conference on File and Storage Technologies Schroeder2007(FAST '07). USENIX Association, Berkeley, CA, USA, , Article 1 .

\bibitem[Dean]{Dean} Jeff Dean. n.y. as found in: Marcos Vaz Salles. PCSD. lecture: Experimental Design - Concurrency Control: 2PL, p.9.

\end{thebibliography}



\end{document}
