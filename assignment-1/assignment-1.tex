\def\Module{Principles of Computer System Design}
\def\Uebung{Assignment 1}
\def\Studentenname{Marcus Voss (qcz284), R. Schmidtke (rxt809), Marco Eilers (dbk726)}
\def\Sub_date{04.12.2012}

\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fullpage} 
\headsep1cm
\parindent0cm
\usepackage{amssymb, amstext, amsmath}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}

\lhead{\textbf{\Module}}
\rhead{\Uebung~(Submission: \Sub_date)}

\cfoot{}
\lfoot{\Studentenname}
\rfoot{\thepage\ of \pageref{LastPage}}
\pagestyle{fancy}
\renewcommand{\footrulewidth}{0.4pt}

\newcommand{\code}[1]{{\fontfamily{fvm}\small \selectfont #1}}

%Line spacing between paragraphs
\setlength{\parskip}{6pt}

\begin{document}

\section*{Exercises} 
\label{sec:exercises}

\subsection*{Question 1}
\label{sec:eq1}

\subsection*{Question 2}
\label{sec:eq2}
Le Voss!

\begin{table}[htbp]
\caption{Comparison of Storage}
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
 & Hard disk & SSD & Main Memory \\ \hline
1 Access time &  &  &  \\ \hline
2 Average Capacity &  &  &  \\ \hline
3 Cost per unit &  &  &  \\ \hline
4 Reliability &  &  &  \\ \hline
5 Power Consumption &  &  &  \\ \hline
\end{tabular}
\end{center}
\label{tab:storage}
\end{table}

\subsection*{Question 3}
\label{sec:eq3}
(a) How does concurrency influence latency in a computer system? Is its influence always positive,
always negative, or is there a trade-off? Explain. (2 paragraphs)
(b) Explain the difference between dallying and batching. Provide one example of each. (2
paragraphs)
(c) Is caching an example of a fast path optimization? Explain why or why not. (1 paragraph)

\subsubsection*{(a)}
While concurrency usually has a positive effect on a system's latency, there may also be cases where this effect is only marginal or even negative. In general, the ability to process several requests at once means that some requests can be worked on right away instead of having to wait for the completion of another request, or at least that qaiting time is reduced. However, this is only true if the system's utilizaztion is high enough. If, on the other hand, the system's utilization is low and it only ever has one request at a time do work with, concurrency will not have a positive effect at all. In this case, it might even have a slightly negative effect, since the added overhead for work distribution and scheduling will add to the latency of every single request.

In the above paragraph, we assumed that the system's hardware natively supports concurrency, which is usually the case nowadays, so that several units of work can actually be executed at the same time. If, however, the system only simulates concurrent execution, the impact on the latency will quite probably always be negative. In this case the system would start working on several tasks at once and distribute the processor's computing time between them. While fewer (or no) tasks would have to wait before they are started, the work on every task will take considerably longer. Combined with the overhead from scheduling etc., the overall latency would be at least as high as with serial execution.

\subsubsection*{(b)}
Batching means performing a number of related tasks at the same time instead of doing each one on its own, usually in order to reduce the overall overhead: Often the overhead for specific tasks stays the same, independent of the size of the task. In this case it can be advantageous to combine several tasks into one, so that the overhead occurs only once and not once for every task. There may be additional performance benefits that arise from similarities or a certain order of the tasks. 

Dallying, on the other hand, means that one delays the execution of a task to get some kind of performance benefit. One reason to do this is because it might not actually be necessary to execute the task after all (but this is not yet clear). The other reason is related to batching: One delays the execution of several small tasks and waits until a certain number of such tasks has been collected. This collection of tasks is then executed at once (using batch execution) in order to profit from the reduced overhead, as described above. Batching may therefore be a part of (or a motivation for) dallying.

Batching is quite common in computing as well as in the real world. TODO

\subsubsection*{(c)}
Caching is a perfect example for fast path optimization. It means that, in addition to the normal and relatively slow way of accessing data, which usually means accessing RAM or a disk or network resource, there is another, much faster way for common requests. This faster way does not work for all kinds of requests (otherwise one would not use the slower one at all), in our case because a cache is typically at least an order of magnitude smaller than the actual memory. A good cache therefore contains those parts of the memory that are accessed most frequently, and getting data from the cache is much faster than getting them from the actual memory. 

\subsection*{Question 4}
\label{sec:eq4}
Question 4 (Experimental Design). [Lilja, partial] Devise an experiment to determine the following
performance metrics for a computer system:
(a) The effective memory bandwidth between the processor and the data cache if all memory
references are cache hits. Make sure to describe in your setup how you manage to ensure that all
references measured are cache hits and why your setup achieves that.
(b) The effective memory bandwidth if all memory references are cache misses. Make sure to
describe in your setup how you manage to ensure that all references measured are cache misses
and why your setup achieves that.


\subsubsection*{(a)}


\subsubsection*{(b)}

\section*{Programming Task}
\label{sec:programming}

\subsection*{Question 1}
\label{sec:pq1}
There are basically three types of semantics: 'Exactly once', 'at most once' and 'at least once'. Since all of these three types rely on the interaction of client and server, it depends mostly on the client what type of semantics is employed (as it may choose to try again and again). The service and client implementations we provide are of the type 'at most once'. If the service is unavailable at the time of the request, the request is not repeated, but an exception is raised. Similarly, exceptions are raised on other errors during execution on the server (as per the \code{KeyValueBase} interface). The clients we ship respect that and do not try again, so the overall semantics used are of type 'at most once'. Other client implementations may prefer to retry the operation on failure until it has succeeded (at least) once which can be implemented just fine by keeping track of errors from the service.

\subsection*{Question 2}
\label{sec:pq2}
Explain the main elements of your implementation of the first version of KeyValueBase.
In particular, describe the mechanisms you used to enforce atomicity of operations (e.g., locks or
monitors) and how you used them. In addition, describe how you implemented free-space management. (2
paragraphs)

We do all memory management and locking and most of the store's logic in IndexImpl, which is a singleton. The current key-value-pairs are managed in a Hashtable, which maps keys to a SpaceIdent object, which in turn contains a value's position and length in the memory mapped file. We also have a list of SpaceIdents which specify all empty areas in the store, i.e. areas that may be overwritten and used by new values. Space is declared to be free when a value is deleted, or when it is updated with a value which does not fit into the currently allocated space. In the latter case, the old space is marked as free and a new free area is found which is large enough to contain the new value. If no such place is found in the emptyList, the new value is spaced at the end of the currently used space. The same happens with inserts. The emptyList is sorted by the SpaceIdent's location. In order to avoid fragmentation of the store, adjacent free areas are joined every time free space is requested (since this requires a traversal of the emptyList anyway). 

We use Java's ReadWriteLock API for locking. The KeyImpl class manages another Hashtable which maps each key to its own ReadWriteLock object, so that each key can be locked on its own and we can distinguish between read and write access. We hope that this granular locking will result in maximal concurrency and therefore maximal performance for our store. When inserting new completely new keys, which do not have a lock yet, we use Java's \texttt{synchronized} on the new key to avoid creating duplicate locks for the same key. In all cases, we acquire all necessary locks at the start of a method and release them again at the end (Conservative Strict 2PL), because we did not want to deal with deadlocks or cascading aborts. For tasks larger than a single key, e.g. atomicScan, we simply acquire the locks for all keys in the order of the keys, while also locking the mapping itself, so that no new keys can be inserted during the operation.

\subsection*{Question 3}
\label{sec:pq3}
To continuously and consistently test our implementation we created a set of unit tests:
\begin{itemize}
\item SimpleReadWriteTest(N): This test simulates only one client that first inserts N values into the key-value store, and does then N sequential random updates and reads. For every read it checks if the values corresponds to a value that is kept locally in a Java Hash Map. The test fails if any read deviates from the expected value.
\item MultiReadTest(N, h, n): This test simulates h clients (threads). First N values are (sequentially) inserted into the key-value store. Then the h clients make n random reads. For every read each thread checks if the values corresponds to a value that is kept in a shared Java Hash Map. The test fails if any read deviates from the expected value.
\item MultiReadWriteTest(N, h, n) This test is similar to MultiReadTest(), but it will also do n random writes and after each write 10 random reads. As above the test fails if any read deviates from the expected value.
\item AtomicUpdateTest()
\item AtomicScanTest()
\end{itemize}


Vossi / Marco

\subsection*{Question 4}
\label{sec:pq4}

\subsection*{Question 5}
\label{sec:pq5}

data set used is\footnote{Downloaded from: \url{http://an.kaist.ac.kr/~haewoon/release/twitter_social_graph/} [last accessed on: November 30, 2012]}

\subsection*{Question 6}
\label{sec:pq6}
Vossi

\subsection*{Question 7}
\label{sec:pq7} 
Vossi

\subsection*{Question 8}
\label{sec:pq8}
Marco

\subsection*{Question 9}
\label{sec:pq9}
Kaniner!

\end{document}
